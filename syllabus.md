# Policies and syllabus

APPM 4490/5490 Theory of Machine Learning.  University of Colorado Boulder, Spring 2024.

More class info (grades, discrimination policies, secret zoom link, piazza link, etc.) are on our [Canvas page](https://canvas.colorado.edu/courses/80449) (sign in via the CU SSO)

[Catalog description](https://catalog.colorado.edu/search/?search=APPM+4490): Presents the underlying theory behind machine learning in proofs-based format. Answers fundamental questions about what learning means and what can be learned via formal models of statistical learning theory. Analyzes some important classes of machine learning methods. Specific topics may include the PAC framework, VC-dimension and Rademacher complexity.

Requires **prerequisite** course of APPM 4440 "Real Analysis" (minimum grade C-).  

For graduate students, there is no enforced prereq, but we suggest mathematical maturity and hopefully an undergraduate or graduate class in analysis, as this is a proofs-based math class.  Familiarity with linear algebra and probability is assumed, and some familiarity with machine learning (like CSCI 5622) is nice but not essential.

## Instructor and contact Information

- The instructor is [Stephen Becker](http://amath.colorado.edu/faculty/becker/), Associate Professor of Applied Mathematics
- Contact him at <stephen.becker@colorado.edu>
- Office: 338 ECOT (engineering center, office tower)
- There is no teaching assistant

### Meeting times and location
Meeting time: MWF 11:15 AM - 12:05 AM

Location: ECCR 135 (Engineering Center)

During times when the class is remote, we will meet via zoom (link is on Canvas)

### Office Hours
3 hours per week, held in a hybrid fashion: attend the physical office (338 ECOT) or via zoom (link is posted in Canvas)

Times:

- Wed 4-5
- Thurs 1-3

There is no TA

### Estimated Workload
This is intended to have less of a workload than a core class, though it will depend a lot on the student's background. This is a three credit course (the standard kind of course).

### Course Format
This is a lecture-based course, and the instructor will present proofs. The textbooks provide supplementary material and homework problems. Students are expected to read the book as necessary to fill in gaps not covered in lecture. Reading the book before lecture is useful but not required. Homeworks will require students to write proofs, synthesizing concepts learned from lecture and the book.

### Homeworks

There will be homeworks, due either weekly or bi-weekly. You are allowed to drop one homework (this will be done automatically).  

### Grading
- 80% homeworks.
  - Late homework is not accepted, but you are allowed one free "dropped" homework. Your lowest-scoring homework will be automatically dropped. See below for caveat
- 10% Midterm (in class)
- 10% Final Project (open-ended)

**There is no in-class final exam.**

The overall grade may be **curved** as appropriate (not guaranteed), but note that there is no set "quota" of A's, B's, etc., so you are not directly competing with your classmates.

**Graduate students** or anyone enrolled in 5490 (instead of 4490) will be given extra homework and/or exam problems, and must do the final project on their own.  Students enrolled in 4490 may form groups of two for the final project.

### Late assignment and cheating policy

In general, **late homework** assignments are not accepted; instead, you can use your one dropped homework.  Under exceptional circumstances (such as serious illness, including COVID-19, or serious family issues), homework can be turned in late.  If the reason is foreseeable (e.g., planned travel), you must contact the instructor in advance.

Examples:

- Your sister is getting married and you have to travel out-of-state.  That's great, but this is when you use the one dropped homework, or turn the homework in early. This is foreseeable, and not an "emergency", so it does not count as an exceptional circumstance.
- A close family member becomes infected with COVID-19 and you have to return to your home country to take care of family.  This *does* count as an exceptional circumstance. Please email the instructor to discuss arrangements.

**Cheating** is not acceptable.  Take-home exams and homeworks are easy to cheat on if you really want to, but as this is an upper-division course, I am relying on the fact that students are here to learn (and paying the university to do so), and thus cheating does not make sense.  Cheating does not hurt the instructor, it hurts the student (and hurts the grades of honest classmates).

If a student is caught cheating, on the first occurrence, the penalty ranges from losing points on the item in question (like one test problem; this is for very minor infractions) to losing all points for the assignment (i.e., the entire homework or entire exam). Students may be referred to the honor council. On the second occurrence of cheating, similar penalties may apply, and additionally the student may fail the class, at the instructor's discretion.

"Minor infractions" include not following the instructions during an exam (in person or remote). For example, if the instructions on a remote test are to keep your microphone on and your hands in sight of your webcam, then failing to follow these instructions construes a minor infraction, and (even though cheating may not be proven) you are subject to losing points.

On homeworks, you are free to **collaborate** with other students, and to use resources like the internet appropriately. However, you must do your own work. There is a gray area between collaboration and cheating, and we rely on the students' and instructors discretion.  Copying code verbatim is never permissible.  You should be writing up your own work, and explaining answers in your own words.  Snippets of code are allowed to be similar (sometimes there is only one good way to do it), but longer chunks of code should never be identical.  If not expressly forbidden by the assignment, you may use the internet, but you may never post for help on online forums.  (Regarding forums, please use our Piazza website if you want a Q&A forum).

Cheating is not usually an issue in this class, and I have faith that students will continue to act appropriately.

### Course website

We will use [github](https://github.com/stephenbeckr/ML-theory-class) for public content (notes, demos, syllabus), and use CU's default LMT **Canvas** for private content like grades and homework solutions.  Canvas will also be used to organize things, like any recorded lectures, comments made via **Gradescope**, and Q&A forums via **piazza**.

### Online behavior
The class is intended to be in person most of the time, but we will use zoom for the first two weeks and may need to switch to zoom later as well depending on COVID-19.

On zoom, please have your webcam on if at all possible

- Valid reasons for not having the camera on: to protect the privacy of your family or roommate, if you cannot have a virtual background
- Invalid reason: you don't feel like it, or you didn't wash your hair.

We have the same standards of behavior as we would in a classroom: appropriate attire, appropriate and not distracting virtual backgrounds, verbal and chat remarks should be respectful, etc.  Real-world backgrounds should be appropriate and professional (please, no drugs or alcohol behind you).

It's always important to have respectful remarks, and even more so in an online setting, since it is easier to get carried away with chat comments since you cannot see the effect on other people.

If we enable private chat on zoom, remember that the zoom host can later see even "private" chats. Inappropriate or inconsiderate remarks, even on private chats, are not allowed.


### Dropping the Course
Advice from your department advisor is recommended before dropping any course. After 11:59 PM Jan. 31, dropping a course results in a "W" on your transcript and you’ll be billed for tuition. After 11:59 PM March 22, dropping the course is possible only with a petition approved by the Dean’s office.

(The last day to *add* a class is Wed Jan. 24)

# Generic policies
For classroom behavior, requirements for COVID-19, accommodation for disabilities, preferred student names and pronouns, honor code, sexual misconduct/discrimination/harassment/retaliation and religious holidays, please refer to the policies document posted on Canvas.


# Syllabus

A proofs based course on the underlying theory behind machine learning. Instead of chasing the latest-greatest algorithm, the course asks fundamental questions about what learning means and what can be learned (by anyone or anything). To answer these questions, formal models of statistical learning theory are used, requiring math, probability, statistics and optimization

### Official course description
Presents the underlying theory behind machine learning in proofs-based format. Answers fundamental questions about what learning means and what can be learned via formal models of statistical learning theory. Analyzes some important classes of machine learning methods. Specific topics may include the PAC framework, VC-dimension and Rademacher complexity.

(Mostly focuses on binary classification, and
almost exclusively focuses on supervised learning and the PAC model.)

### Related courses at CU
- [CSCI 6622 Machine Learning](https://www.colorado.edu/cs/csci-5622-machine-learning) has more focus on practical machine learning.
- [ECEN-5002-002 Special Topics: Machine Learning for Engineers (or ECEN 5712 now)](https://sites.google.com/colorado.edu/machine-learning-for-engineers/) taught by Profs. Eugene Liu and Mahesh Varanasi. This course is more similar to this course, as it uses the Shalev-Shwartz and Ben-David book as well.

### Textbooks
- The **main textbook** is [Understanding Machine Learning: From Theory to Algorithms](https://www.amazon.com/dp/B00J8LQU8I), 1st edition, by Shai Shalev-Shwartz and Shai Ben-David (Cambridge University Press, 2014, ISBN-13: 978-1107057135,
$35–$45 on Amazon). There is a free preprint [PDF version of the book on the authors' website](https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/copy.html)

- As a **supplemental text**, we will use [Foundations of Machine Learning](https://www.amazon.com/dp/0262039400/), 2nd edition, by Mehryar Mohri,
Afshin Rostamizadeh, and Ameet Talwalkar (MIT Press, 2018, ISBN-13: 978-0262039406, $50 on
Amazon). The authors host a [free PDF of the book at their website](https://cs.nyu.edu/~mohri/mlbook/).  We'll use this book more for the second half of the semester.

- For **probability background**, we use [High-Dimensional Probability](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html) by Roman Vershynin, Cambridge University Pres (2018).

While we won't make much use of them, there are lots of other good mathematical texts out there, such as

- “Introduction to Statistical Learning Theory,” Bousquet, Boucheron and Lugosi (2003). 39 pages. Available on the [internet](http://www.econ.upf.edu/~lugosi/mlss_slt.pdf), including via [SpringerLink](https://link.springer.com/chapter/10.1007/978-3-540-28650-9_8).
- “The Nature of Statistical Learning Theory,” Vapnik, 2nd edition (2000). Available via [SpringerLink](https://link.springer.com/book/10.1007%2F978-1-4757-3264-1).

As well as newer texts that often cover the above standard material and also try to add what (little) theory is known about tight characterizations of deep nets:
- [The Principles of Deep Learning Theory](https://arxiv.org/pdf/2106.10165) by Daniel A. Roberts and Sho Yaida, 2021, based on research in collaboration with Boris Hanin
- [STATS214 / CS229M: Machine Learning Theory](https://web.stanford.edu/class/stats214/) at Stanford, 2021, with [github notes](https://github.com/tengyuma/cs229m_notes/blob/main/master.pdf) by Tengyu Ma 2022.
- [Learning Theory from First Principles](https://www.di.ens.fr/~fbach/ltfp_book.pdf) by Francis Bach, 2024


### Principal Topics
PAC framework, VC-dimension, Rademacher complexity, bias-variance tradeoffs, no-free lunch theorems, model selection, boosting, expressive power

### Learning Goals/Outcomes
[//]: # ( Not testable; high-level )
[//]: # ( Learning Objectives, i.e., quantifiable outcomes )
[//]: # ( Something measurable )
The aim of the course is to show students a few possible frameworks in which they can formally analyze machine learning algorithms. Since machine learning is hyped by the media, it is important for students to understand fundamental limits as to what is possible. Furthermore, students will gain an understanding of the assumptions and limitations of the models, and when the theoretical bounds
are not useful.

### Learning Objectives
After taking the course, students will be able to understand the standard frameworks, and analyze standard algorithms within this framework. Students will understand key terms as used in the field, like bias, variance, expressiveness, learnability, training, and generalization. Students will be be able to identify when analysis is possible or impossible. Students will understand that alternative learning models and tasks exist, and know when each model is most appropriate, and in particular be able to evaluate the assumptions and shortcomings of a given model.


### Programming
There may be a bit of programming involved in some homeworks. Any reasonable language (e.g., Python, R, Matlab, Julia) is acceptable.

### More details on Topics
To see what we actually cover, see the [Lectures.md](Lectures.md) document; you can see what was covered last time we taught this in [Lectures2020.md](Lectures2020.md).

We mostly follow the Shalev-Shwartz and Ben-David book, skipping a few chapters and adding a few chapters from Mohri et al.

**Classical Statistical Learning Theory** We mainly focus on supervised statistical batch learning with a passive learner.

1. Ch 1: Intro to class: what is it about?
2. Ch 2: Formal models (statistical learning), Empirical Risk Minimization (ERM), finite hypothesis class
3. Ch 3: Formal Learning model: Probably-Almost-Correct (PAC)
4. Ch 4: Learning via Uniform Convergence (and concentration inequalities, cf Appendix B and Vershynin)
5. Ch 5: Bias-Complexity Tradeoff, double-descent, no-free-lunch theorems
6. Ch 6: VC-Dimension
7. Ch 26: Rademacher Complexity (and ch 3.1 in Mohri)
8. Ch 27: Covering Numbers

**Analysis of Algorithms** As time permits, we will analyze standard algorithms.

1. Ch 9: Linear predictors
2. Ch 10: Boosting, AdaBoost
3. Ch 11: Model selection and validation
4. Ch 12: Convex learning problems (generalization bounds)
5. Ch 13: Regularization and Stability
6. Ch 15: Support Vector Machines (SVM)
7. Ch 16: Kernel methods
8. Ch 20: Neural Networks, expressive power, and new results about deep networks (2017–now)

**Additional Topics** We will cover these as we have time

1. Ch 21: Online Learning
2. Reinforcement learning (ch 17 in Mohri)
3. Background on Information Theory (Appendix E in Mohri)
4. Max Entropy (ch 12 in Mohri)
5. Ch 22: Clustering (K-means, spectral clustering, information bottleneck)
6. Ch 7: Nonuniform Learnability
7. Computational Complexity models (Turing Machines; see Scott Aaronson book)
8. Ch 8: Computational Complexity of learning
9. Ch 14: Stochastic Gradient Descent
10. More stats, e.g., Expectation Maximization
10. Variational Inference, ELBO
11. Information Theory, information bottleneck
11. Generative Models (GANS, Variational AutoEncoders)
10. Recent papers from the literature
